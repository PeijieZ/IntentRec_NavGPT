{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53949b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "import googlemaps\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04eeee5b-e673-457d-9cf7-303e0f4d62c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = 'goals_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f93d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Location: Bristol, UK\n",
      "Goals List: Derby, UK\n",
      "observations: [[51.4551987, -2.5868899], [51.4597708, -2.5840761], [52.3522323, -1.809647], [52.9488481, -1.1506018], [52.95400799999999, -1.1552171]]\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize variables\n",
    "initial_location = data[1]['initial']\n",
    "goals_list = data[1]['goals']\n",
    "observations = data[2]['observations']\n",
    "\n",
    "# Print or use the variables as needed\n",
    "print(\"Initial Location:\", initial_location)\n",
    "print(\"Goals List:\", goals_list[1])\n",
    "print(\"observations:\", observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "265aa3ed-cd83-4408-b2a7-1ae25a3c1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_similarity(route1, route2):\n",
    "#     path1 = route1  # Assuming route1 is a list of coordinates\n",
    "#     path2 = route2  # Assuming route2 is a list of coordinates\n",
    "\n",
    "#     if len(path1) == 0 and len(path2) == 0:\n",
    "#         return 1000  # If either path is empty, consider them 100% similar\n",
    "\n",
    "#     common_points = [point1 for point1 in path1 if any(are_points_equal(point1, point2) for point2 in path2)]\n",
    "\n",
    "#     similarity_percentage = (len(common_points) / max(len(path1), len(path2))) * 100\n",
    "\n",
    "#     # if similarity_percentage > 100:\n",
    "#     #     return 100\n",
    "\n",
    "#     return similarity_percentage\n",
    "\n",
    "# def are_points_equal(point1, point2):\n",
    "#     epsilon = 1e-5\n",
    "#     lat_diff = abs(point1[0] - point2[0])\n",
    "#     lng_diff = abs(point1[1] - point2[1])\n",
    "\n",
    "#     return lat_diff < epsilon and lng_diff < epsilon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32342c2d-0a74-42b9-9eac-1af1c3031a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_paths(path1, path2):\n",
    "    length1 = len(path1)\n",
    "    length2 = len(path2)\n",
    "\n",
    "    if length1 == length2:\n",
    "        return path1, path2  # Paths already have the same length\n",
    "\n",
    "    normalized_path1 = path1.copy()\n",
    "    normalized_path2 = path2.copy()\n",
    "\n",
    "    # Interpolate additional points along the shorter path\n",
    "    if length1 < length2:\n",
    "        normalized_path1 = interpolate_path(path1, length2)\n",
    "    else:\n",
    "        normalized_path2 = interpolate_path(path2, length1)\n",
    "\n",
    "    return normalized_path1, normalized_path2\n",
    "\n",
    "def interpolate_path(path, target_length):\n",
    "    interpolated_path = []\n",
    "\n",
    "    for i in range(target_length):\n",
    "        index = math.floor((i / (target_length - 1)) * (len(path) - 1))\n",
    "        interpolated_path.append(path[index])\n",
    "\n",
    "    return interpolated_path\n",
    "\n",
    "def calculate_distance(point1, point2):\n",
    "    R = 6371e3  # Earth radius in meters\n",
    "    lat1, lon1 = point1\n",
    "    lat2, lon2 = point2\n",
    "    φ1 = math.radians(lat1)\n",
    "    φ2 = math.radians(lat2)\n",
    "    Δφ = math.radians(lat2 - lat1)\n",
    "    Δλ = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(Δφ / 2) * math.sin(Δφ / 2) + math.cos(φ1) * math.cos(φ2) * math.sin(Δλ / 2) * math.sin(Δλ / 2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    return R * c  # Distance in meters\n",
    "\n",
    "def calculate_similarity(route1, route2):\n",
    "    normalized_route1, normalized_route2 = normalize_paths(route1, route2)\n",
    "\n",
    "    max_length = max(len(normalized_route1), len(normalized_route2))\n",
    "    total_distance = 0\n",
    "    similar_points = 0\n",
    "\n",
    "    for i in range(max_length):\n",
    "        point1 = normalized_route1[math.floor(i * len(normalized_route1) / max_length)]\n",
    "        point2 = normalized_route2[math.floor(i * len(normalized_route2) / max_length)]\n",
    "        distance = calculate_distance(point1, point2)\n",
    "        total_distance += distance\n",
    "        if distance < 45:  # You can adjust this threshold according to your needs\n",
    "            similar_points += 1\n",
    "\n",
    "    similarity_percentage = (similar_points / max_length) * 100\n",
    "\n",
    "    return min(similarity_percentage, 100)  # Ensure similarity doesn't exceed 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb2638-9be8-4813-a2d2-3381e34a8dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a7eca-0dc4-4484-8349-1d9e94c3ab1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efcc803a-7734-4155-82a4-ef3bcbf3ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"locations_with_lat_lng.txt\"\n",
    "df_path = pd.read_csv(file_path)\n",
    "\n",
    "def get_lat_lng(location):\n",
    "    # Find the row corresponding to the input location\n",
    "    row = df_path[df_path['Locations'] == location]\n",
    "\n",
    "    # If location not found in DataFrame, return None\n",
    "    if row.empty:\n",
    "        return None, None\n",
    "    \n",
    "    # Extract latitude and longitude from the DataFrame\n",
    "    return row['Latitude'].values[0], row['Longitude'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233619f2-364c-4439-b7ae-2d79b0e69566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c16c3e0-85fe-49a9-bf6d-6dbf522b2b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "datalength = len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56444373-c2bc-4329-a65b-4bb01e8947bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1 Bristol, UK Nottingham, UK\n",
      "1.2.1 Bristol, UK Derby, UK\n"
     ]
    }
   ],
   "source": [
    "for current_id in range(3):\n",
    "    goals = data[current_id]['goals']\n",
    "    if current_id % 5 == 0: \n",
    "            for goal in range(len(goals)):\n",
    "                print(data[current_id]['id'], data[current_id]['initial'], goals[goal])\n",
    "    \n",
    "    # for goal in range(len(goals)):\n",
    "    #     print(data[current_id]['id'])\n",
    "        # route_points = calculate_route_points(data[current_id]['initial'], goals[goal])\n",
    "        # print(data[current_id]['id'],data[current_id]['initial'], goals[goal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae168c86-d956-4aa8-b8b4-5c9d9b58bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data_output'\n",
    "\n",
    "similarity_ai_csv_path = os.path.join(folder_path, 'similarity_ai.csv')\n",
    "df_similarity_ai = pd.read_csv(similarity_ai_csv_path, index_col='ID')\n",
    "similarity_file_ai = df_similarity_ai.to_dict(orient='index')\n",
    "\n",
    "similarity_google_csv_path = os.path.join(folder_path, 'similarity_google.csv')\n",
    "df_similarity_google = pd.read_csv(similarity_google_csv_path, index_col='ID')\n",
    "similarity_file_google = df_similarity_google.to_dict(orient='index')\n",
    "\n",
    "similarity_mapbox_csv_path = os.path.join(folder_path, 'similarity_mapbox.csv')\n",
    "df_similarity_mapbox = pd.read_csv(similarity_mapbox_csv_path, index_col='ID')\n",
    "similarity_file_mapbox = df_similarity_mapbox.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db4d05f7-d798-4032-8493-321d63c4d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_file_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a783d982-e16c-40c1-a380-6f570de0d182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1 "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'similarity_file_ai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[current_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m goal \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(goals)):\n\u001b[0;32m---> 19\u001b[0m     route_similarity \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity_file_ai\u001b[49m[data[current_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]][\u001b[38;5;28mstr\u001b[39m(goal)]\n\u001b[1;32m     21\u001b[0m     route_similarity_array\u001b[38;5;241m.\u001b[39mappend(route_similarity)\n\u001b[1;32m     23\u001b[0m     point_distance \u001b[38;5;241m=\u001b[39m calculate_distance(get_lat_lng(goals[goal]), data[current_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'similarity_file_ai' is not defined"
     ]
    }
   ],
   "source": [
    "similarity_dic_ai = {}\n",
    "intent_dic_ai = {}\n",
    "\n",
    "\n",
    "for current_id in range(datalength):\n",
    "    intent = {}\n",
    "\n",
    "    route_similarity_array = []\n",
    "\n",
    "    point_distance_array = []\n",
    "    \n",
    "    goals = data[current_id]['goals']\n",
    "    \n",
    "    print(data[current_id]['id'], end=' ')\n",
    "    \n",
    "    for goal in range(len(goals)):\n",
    "        \n",
    "        \n",
    "        route_similarity = similarity_file_ai[data[current_id]['id']][str(goal)]\n",
    "\n",
    "        route_similarity_array.append(route_similarity)\n",
    "\n",
    "        point_distance = calculate_distance(get_lat_lng(goals[goal]), data[current_id]['observations'][-1])\n",
    "        # print(point_distance)\n",
    "        point_distance_array.append(point_distance)\n",
    "\n",
    "    \n",
    "    similarity_dic_ai[data[current_id]['id']] = route_similarity_array \n",
    "    total_distance = sum(point_distance_array)\n",
    "\n",
    "        # print(goals[goal], format(route_similarity, '.2f'))\n",
    "        \n",
    "    for i in range(len(goals)):\n",
    "        intent_percentage = (route_similarity_array[i] / sum(route_similarity_array)) * 100\n",
    "        h1_intent_percentage = (point_distance_array[i] / total_distance) * 100\n",
    "    \n",
    "        # Calculate the combined intent as described\n",
    "        combined_intent_percentage = 0.9 * intent_percentage + 0.1 * h1_intent_percentage\n",
    "        \n",
    "        intent[goals[i]] = format(intent_percentage, '.2f')\n",
    "    \n",
    "    intent_dic_ai[data[current_id]['id']] = intent\n",
    "\n",
    "#     print(intent)\n",
    "# intent_dic_ai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58640b49-705f-4079-bba2-1798a5e801f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counts\n",
    "TP_ai = 0\n",
    "FN_ai = 0\n",
    "FP_ai = 0\n",
    "TN_ai = 0\n",
    "ai_intention = {}\n",
    "\n",
    "count = 0  # Reset count for each iteration\n",
    "\n",
    "for id in intent_dic_ai:\n",
    "    highest_intent, highest_score = max(intent_dic_ai[id].items(), key=lambda x: float(x[1]))\n",
    "    \n",
    "    # Find all scores that are equal to the highest score\n",
    "    highest_scores = [(key, value) for key, value in intent_dic_ai[id].items() if value == highest_score]\n",
    "    point_distance_array = []\n",
    "    \n",
    "    if len(highest_scores) >= 2:\n",
    "        \n",
    "        for location, score in highest_scores:\n",
    "            # print(location)\n",
    "            lat_lng = get_lat_lng(location)\n",
    "            if lat_lng is not None:\n",
    "              \n",
    "                if len(data[count]['observations']) == 1:\n",
    "                    # Calculate point distance differently in this case\n",
    "                    point_distance1 = calculate_distance(lat_lng, get_lat_lng(data[count]['initial']))\n",
    "                    point_distance2 = calculate_distance(lat_lng, data[count]['observations'][0])  # Use the single observation\n",
    "                    point_distance = point_distance2 - point_distance1\n",
    "                    point_distance_array.append((location, point_distance))\n",
    "                else:\n",
    "                    # Calculate point distance using the first and last observations\n",
    "                    point_distance1 = calculate_distance(lat_lng, data[count]['observations'][0])\n",
    "                    point_distance2 = calculate_distance(lat_lng, data[count]['observations'][-1])\n",
    "                    point_distance = point_distance2 - point_distance1\n",
    "                    point_distance_array.append((location, point_distance))\n",
    "                \n",
    "               \n",
    "        shortest_distance_location = min(point_distance_array, key=lambda x: x[1])[0]\n",
    "        highest_score_location = [loc for loc, _ in highest_scores]\n",
    "        if shortest_distance_location in highest_score_location:\n",
    "            highest_intent = shortest_distance_location\n",
    "            # print(highest_intent)\n",
    "\n",
    "    # Check if the highest scoring intent matches the intent_goal\n",
    "    if highest_intent == data[count]['intent_goal']:\n",
    "        ai_intention[id] = 1\n",
    "        TP_ai += 1\n",
    "        TN_ai += len(intent_dic_ai[id]) - 1\n",
    "    else:\n",
    "        ai_intention[id] = 0\n",
    "        FN_ai += 1\n",
    "        FP_ai += len(intent_dic_ai[id]) - 1\n",
    "    \n",
    "    count += 1   \n",
    "\n",
    "# Calculate evaluation metrics, handle division by zero cases\n",
    "TPR_recall_ai = TP_ai / (TP_ai + FN_ai)  # Ensuring the denominator is not zero\n",
    "Precision_ai = TP_ai / (TP_ai + FP_ai)  # Ensuring the denominator is not zero\n",
    "FNR_ai = FN_ai / (FN_ai + TP_ai)  # Ensuring the denominator is not zero\n",
    "\n",
    "# Calculate FPR, handle division by zero cases\n",
    "FPR_ai = FP_ai / (FP_ai + TN_ai)  # Ensuring the denominator is not zero\n",
    "\n",
    "# Calculate F1 score, handle division by zero cases\n",
    "denominator = Precision_ai + TPR_recall_ai  # Ensuring the denominator is not zero\n",
    "F1_ai = (2 * Precision_ai * TPR_recall_ai) / denominator\n",
    "\n",
    "# Print or use the evaluation metrics as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594bf52-443d-4f7c-8096-c4f1576a5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR_recall_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4317932-e72c-4ac8-98bf-02f538916404",
   "metadata": {},
   "outputs": [],
   "source": [
    "FNR_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008e84e-5a1a-46b0-929d-d6f9d75e99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac85eeed-a2ff-4309-8276-5a8ed1d7e82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1 1.2.3 1.2.5 1.2.10 1.2.15 1.5.1 1.5.3 1.5.5 1.5.10 1.5.15 1.10.1 1.10.3 1.10.5 1.10.10 1.10.15 1.15.1 1.15.3 1.15.5 1.15.10 1.15.15 2.2.1 2.2.3 2.2.5 2.2.10 2.2.15 2.5.1 2.5.3 2.5.5 2.5.10 2.5.15 2.10.1 2.10.3 2.10.5 2.10.10 2.10.15 2.15.1 2.15.3 2.15.5 2.15.10 2.15.15 3.2.1 3.2.3 3.2.5 3.2.10 3.2.15 3.5.1 3.5.3 3.5.5 3.5.10 3.5.15 3.10.1 3.10.3 3.10.5 3.10.10 3.10.15 3.15.1 3.15.3 3.15.5 3.15.10 3.15.15 4.2.1 4.2.3 4.2.5 4.2.10 4.2.15 4.5.1 4.5.3 4.5.5 4.5.10 4.5.15 4.10.1 4.10.3 4.10.5 4.10.10 4.10.15 4.15.1 4.15.3 4.15.5 4.15.10 4.15.15 5.2.1 5.2.3 5.2.5 5.2.10 5.2.15 5.5.1 5.5.3 5.5.5 5.5.10 5.5.15 5.10.1 5.10.3 5.10.5 5.10.10 5.10.15 5.15.1 5.15.3 5.15.5 5.15.10 5.15.15 "
     ]
    }
   ],
   "source": [
    "similarity_dic_google = {}\n",
    "intent_dic_google = {}\n",
    "\n",
    "for current_id in range(datalength):\n",
    "    intent = {}\n",
    "    route_similarity_array = []\n",
    "    point_distance_array = []\n",
    "    \n",
    "    goals = data[current_id]['goals']\n",
    "    \n",
    "    print(data[current_id]['id'], end=' ')\n",
    "    \n",
    "    for goal in range(len(goals)):\n",
    "        route_similarity = similarity_file_google[data[current_id]['id']][str(goal)]\n",
    "        route_similarity_array.append(route_similarity)\n",
    "\n",
    "        if len(data[current_id]['observations']) == 1:\n",
    "            # Calculate point distance differently in this case\n",
    "            point_distance1 = calculate_distance(get_lat_lng(goals[goal]), get_lat_lng(data[current_id]['intent_goal']))\n",
    "            point_distance2 = calculate_distance(get_lat_lng(goals[goal]), data[current_id]['observations'][0])  # Use the single observation\n",
    "            point_distance = point_distance2 - point_distance1\n",
    "            point_distance_array.append(point_distance)\n",
    "        else:\n",
    "            # Calculate point distance using the first and last observations\n",
    "            point_distance1 = calculate_distance(get_lat_lng(goals[goal]), data[current_id]['observations'][0])\n",
    "            point_distance2 = calculate_distance(get_lat_lng(goals[goal]), data[current_id]['observations'][-1])\n",
    "            point_distance = point_distance2 - point_distance1\n",
    "            point_distance_array.append(point_distance)\n",
    "\n",
    "    similarity_dic_google[data[current_id]['id']] = route_similarity_array \n",
    "\n",
    "    total_distance = sum(point_distance_array)\n",
    "    min_distance = min(point_distance_array)\n",
    "    max_distance = max(point_distance_array)  # Added a missing closing parenthesis here\n",
    "\n",
    "    h2_intent_percentages = [100 - ((distance - min_distance) / (max_distance - min_distance) * 100) for distance in point_distance_array]\n",
    "\n",
    "    # Normalize h2_intent_percentages so that their sum is 100%\n",
    "    total_h2_intent_percentage = sum(h2_intent_percentages)\n",
    "    if total_h2_intent_percentage != 0:  # Avoid division by zero\n",
    "        h2_intent_percentages = [percentage * 100 / total_h2_intent_percentage for percentage in h2_intent_percentages]\n",
    "        \n",
    "    for i in range(len(goals)):\n",
    "        intent_percentage = (route_similarity_array[i] / sum(route_similarity_array)) * 100\n",
    "        # Use h2_intent_percentages for normalization\n",
    "        combined_intent_percentage = 0.9 * intent_percentage + 0.1 * h2_intent_percentages[i]  # Use h2_intent_percentages here\n",
    "        intent[goals[i]] = format(intent_percentage, '.2f')  # Use combined_intent_percentage here\n",
    "    \n",
    "    intent_dic_google[data[current_id]['id']] = intent\n",
    "\n",
    "#     print(intent)\n",
    "# intent_dic_ai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1c95ca6-b390-4648-898f-b128efdef242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "TP_google = 0\n",
    "FN_google = 0\n",
    "FP_google = 0\n",
    "TN_google = 0\n",
    "for id in intent_dic_google:\n",
    "    highest_intent, highest_score = max(intent_dic_google[id].items(), key=lambda x: float(x[1]))\n",
    "    \n",
    "    # Check if the highest scoring intent matches the intent_goal\n",
    "    if highest_intent == data[count]['intent_goal']:\n",
    "        TP_google += 1\n",
    "        TN_google += len(intent_dic_google[id]) - 1\n",
    "    else:\n",
    "        FN_google += 1\n",
    "        FP_google += len(intent_dic_google[id]) - 1\n",
    "    \n",
    "    count += 1   \n",
    "TPR_recall_google = TP_google/(TP_google + FN_google)\n",
    "Precision_google = TP_google/(TP_google + FP_google)\n",
    "FNR_google = FN_google/(FN_google + TP_google)\n",
    "FPR_google = FP_google/max(FP_google + TN_google, 1)\n",
    "\n",
    "F1_google = (2*Precision_google*TPR_recall_google)/(Precision_google + TPR_recall_google)\n",
    "TPR_recall_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c78d12e0-d2c8-4913-9b73-2b8d1bc22217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "TP_google = 0\n",
    "FN_google = 0\n",
    "FP_google = 0\n",
    "TN_google = 0\n",
    "for id in intent_dic_google:\n",
    "    highest_score = max(intent_dic_google[id].items(), key=lambda x: float(x[1]))[1]\n",
    "    \n",
    "    # Find all scores that are equal to the highest score\n",
    "    highest_scores = [(key, value) for key, value in intent_dic_google[id].items() if value == highest_score]\n",
    "    # Check if the intent goal matches any of the highest scoring intents\n",
    "    matched = False\n",
    "    for score in highest_scores:\n",
    "        if score[0] == data[count]['intent_goal']:\n",
    "            TP_google += 1\n",
    "            TN_google += len(intent_dic_google[id]) - 1\n",
    "            matched = True\n",
    "            break\n",
    "    \n",
    "    # If the intent goal does not match any of the highest scoring intents\n",
    "    if not matched:\n",
    "        FN_google += 1\n",
    "        FP_google += len(intent_dic_google[id]) - 1\n",
    "    \n",
    "    count += 1   \n",
    "TPR_recall_google = TP_google/(TP_google + FN_google)\n",
    "Precision_google = TP_google/(TP_google + FP_google)\n",
    "FNR_google = FN_google/(FN_google + TP_google)\n",
    "FPR_google = FP_google/max(FP_google + TN_google, 1)\n",
    "\n",
    "F1_google = (2*Precision_google*TPR_recall_google)/(Precision_google + TPR_recall_google)\n",
    "TPR_recall_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c36b168-d5b8-49ce-9892-573f70507dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "TP_google = 0\n",
    "FN_google = 0\n",
    "FP_google = 0\n",
    "TN_google = 0\n",
    "google_intention = {}\n",
    "\n",
    "for id in intent_dic_google:\n",
    "    highest_intent, highest_score = max(intent_dic_google[id].items(), key=lambda x: float(x[1]))\n",
    "    \n",
    "    # Find all scores that are equal to the highest score\n",
    "    highest_scores = [(key, value) for key, value in intent_dic_google[id].items() if value == highest_score]\n",
    "    point_distance_array = []\n",
    "    \n",
    "    if len(highest_scores) >= 2:\n",
    "        \n",
    "        for location, score in highest_scores:\n",
    "            # print(location)\n",
    "            lat_lng = get_lat_lng(location)\n",
    "            if lat_lng is not None:\n",
    "              \n",
    "                if len(data[count]['observations']) == 1:\n",
    "                    # Calculate point distance differently in this case\n",
    "                    point_distance1 = calculate_distance(lat_lng, get_lat_lng(data[count]['initial']))\n",
    "                    point_distance2 = calculate_distance(lat_lng, data[count]['observations'][0])  # Use the single observation\n",
    "                    point_distance = point_distance2 - point_distance1\n",
    "                    point_distance_array.append((location, point_distance))\n",
    "                else:\n",
    "                    # Calculate point distance using the first and last observations\n",
    "                    point_distance1 = calculate_distance(lat_lng, data[count]['observations'][0])\n",
    "                    point_distance2 = calculate_distance(lat_lng, data[count]['observations'][-1])\n",
    "                    point_distance = point_distance2 - point_distance1\n",
    "                    point_distance_array.append((location, point_distance))\n",
    "                \n",
    "               \n",
    "        shortest_distance_location = min(point_distance_array, key=lambda x: x[1])[0]\n",
    "        highest_score_location = [loc for loc, _ in highest_scores]\n",
    "        if shortest_distance_location in highest_score_location:\n",
    "            highest_intent = shortest_distance_location\n",
    "            # print(highest_intent)\n",
    "\n",
    "    # Check if the highest scoring intent matches the intent_goal\n",
    "    if highest_intent == data[count]['intent_goal']:\n",
    "        google_intention[id] = 1\n",
    "        TP_google += 1\n",
    "        TN_google += len(intent_dic_google[id]) - 1\n",
    "    else:\n",
    "        # print(data[count]['id'], \" \")\n",
    "        google_intention[id] = 0\n",
    "        FN_google += 1\n",
    "        FP_google += len(intent_dic_google[id]) - 1\n",
    "    \n",
    "    count += 1  \n",
    "    \n",
    "TPR_recall_google = TP_google/(TP_google + FN_google)\n",
    "Precision_google = TP_google/(TP_google + FP_google)\n",
    "FNR_google = FN_google/(FN_google + TP_google)\n",
    "FPR_google = FP_google/max(FP_google + TN_google, 1)\n",
    "\n",
    "F1_google = (2*Precision_google*TPR_recall_google)/(Precision_google + TPR_recall_google)\n",
    "# TPR_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5283fe3-f567-468a-abf2-52b1d101f523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TPR_recall_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436b5a4-2fd3-4c7e-8e77-9fcfcc247464",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dic_mapbox = {}\n",
    "intent_dic_mapbox = {}\n",
    "\n",
    "\n",
    "for current_id in range(datalength):\n",
    "    intent = {}\n",
    "\n",
    "    route_similarity_array = []\n",
    "\n",
    "    point_distance_array = []\n",
    "    \n",
    "    goals = data[current_id]['goals']\n",
    "    \n",
    "    print(data[current_id]['id'], end=' ')\n",
    "    \n",
    "    for goal in range(len(goals)):\n",
    "        \n",
    "        \n",
    "        route_similarity = similarity_file_mapbox[data[current_id]['id']][str(goal)]\n",
    "\n",
    "        route_similarity_array.append(route_similarity)\n",
    "\n",
    "        point_distance = calculate_distance(get_lat_lng(goals[goal]), data[current_id]['observations'][-1])\n",
    "        # print(point_distance)\n",
    "        point_distance_array.append(point_distance)\n",
    "\n",
    "    \n",
    "    similarity_dic_mapbox[data[current_id]['id']] = route_similarity_array \n",
    "    total_distance = sum(point_distance_array)\n",
    "\n",
    "        # print(goals[goal], format(route_similarity, '.2f'))\n",
    "        \n",
    "    for i in range(len(goals)):\n",
    "        intent_percentage = (route_similarity_array[i] / sum(route_similarity_array)) * 100\n",
    "        h1_intent_percentage = (point_distance_array[i] / total_distance) * 100\n",
    "    \n",
    "        # Calculate the combined intent as described\n",
    "        combined_intent_percentage = 0.9 * intent_percentage + 0.1 * h1_intent_percentage\n",
    "        \n",
    "        intent[goals[i]] = format(intent_percentage, '.2f')\n",
    "    \n",
    "    intent_dic_mapbox[data[current_id]['id']] = intent\n",
    "\n",
    "#     print(intent)\n",
    "# intent_dic_mapbox "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3faae14-5711-4ff1-8c83-f273b20f49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counts\n",
    "TP_mapbox = 0\n",
    "FN_mapbox = 0\n",
    "FP_mapbox = 0\n",
    "TN_mapbox = 0\n",
    "mapbox_intention = {}\n",
    "\n",
    "count = 0  # Reset count for each iteration\n",
    "\n",
    "for id in intent_dic_mapbox:\n",
    "    highest_intent, highest_score = max(intent_dic_mapbox[id].items(), key=lambda x: float(x[1]))\n",
    "    \n",
    "    # Find all scores that are equal to the highest score\n",
    "    highest_scores = [(key, value) for key, value in intent_dic_mapbox[id].items() if value == highest_score]\n",
    "    point_distance_array = []\n",
    "    \n",
    "    if len(highest_scores) >= 2:\n",
    "        \n",
    "        for location, score in highest_scores:\n",
    "            # print(location)\n",
    "            lat_lng = get_lat_lng(location)\n",
    "            if lat_lng is not None:\n",
    "              \n",
    "                if len(data[count]['observations']) == 1:\n",
    "                    # Calculate point distance differently in this case\n",
    "                    point_distance1 = calculate_distance(lat_lng, get_lat_lng(data[count]['initial']))\n",
    "                    point_distance2 = calculate_distance(lat_lng, data[count]['observations'][0])  # Use the single observation\n",
    "                    point_distance = point_distance2 - point_distance1\n",
    "                    point_distance_array.append((location, point_distance))\n",
    "                else:\n",
    "                    # Calculate point distance using the first and last observations\n",
    "                    point_distance1 = calculate_distance(lat_lng, data[count]['observations'][0])\n",
    "                    point_distance2 = calculate_distance(lat_lng, data[count]['observations'][-1])\n",
    "                    point_distance = point_distance2 - point_distance1\n",
    "                    point_distance_array.append((location, point_distance))\n",
    "                \n",
    "               \n",
    "        shortest_distance_location = min(point_distance_array, key=lambda x: x[1])[0]\n",
    "        highest_score_location = [loc for loc, _ in highest_scores]\n",
    "        if shortest_distance_location in highest_score_location:\n",
    "            highest_intent = shortest_distance_location\n",
    "            # print(highest_intent)\n",
    "\n",
    "    # Check if the highest scoring intent matches the intent_goal\n",
    "    if highest_intent == data[count]['intent_goal']:\n",
    "        mapbox_intention[id] = 1\n",
    "        TP_mapbox += 1\n",
    "        TN_mapbox += len(intent_dic_ai[id]) - 1\n",
    "    else:\n",
    "        mapbox_intention[id] = 0\n",
    "        FN_mapbox += 1\n",
    "        FP_mapbox += len(intent_dic_ai[id]) - 1\n",
    "    \n",
    "    count += 1  \n",
    "\n",
    "# Calculate evaluation metrics, handle division by zero cases\n",
    "TPR_recall_mapbox = TP_mapbox / (TP_mapbox + FN_mapbox)  # Ensuring the denominator is not zero\n",
    "Precision_mapbox = TP_mapbox / (TP_mapbox + FP_mapbox)  # Ensuring the denominator is not zero\n",
    "FNR_mapbox = FN_mapbox / (FN_mapbox + TP_mapbox)  # Ensuring the denominator is not zero\n",
    "\n",
    "# Calculate FPR, handle division by zero cases\n",
    "FPR_mapbox = FP_mapbox / (FP_mapbox + TN_mapbox)  # Ensuring the denominator is not zero\n",
    "\n",
    "# Calculate F1 score, handle division by zero cases\n",
    "denominator = Precision_mapbox + TPR_recall_mapbox  # Ensuring the denominator is not zero\n",
    "F1_mapbox = (2 * Precision_mapbox * TPR_recall_mapbox) / denominator\n",
    "\n",
    "# Print or use the evaluation metrics as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3271c4-e714-49df-9423-1182a377be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "data_df = {\n",
    "    'Metric': ['No. Problem','TPR', 'FPR', 'FNR', 'F1-score'],\n",
    "    'LLM': [len(intent_dic_ai), TPR_recall_ai, FPR_ai, FNR_ai, F1_ai],\n",
    "    'Mapbox': [len(intent_dic_mapbox), TPR_recall_mapbox, FPR_mapbox, FNR_mapbox, F1_mapbox],\n",
    "    'Google': [len(intent_dic_google), TPR_recall_google, FPR_google, FNR_google, F1_google]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data_df)\n",
    "\n",
    "# Set 'Metric' column as index\n",
    "df.set_index('Metric', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5dd25-558a-4afd-bd56-1a732308c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T\n",
    "folder_path = 'data_output'\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Write DataFrame to a CSV file in the specified folder\n",
    "# df.T.to_csv(os.path.join(folder_path, 'result_dataframe_overall.csv'))\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9985c4-1590-40ca-9b1c-a250af68402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent_dic_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f5385b-3ab1-4e0d-8527-8f02e106e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_data = [[] for _ in range(4)]\n",
    "\n",
    "for id in intent_dic_ai:\n",
    "    # Find the highest scoring intent\n",
    "\n",
    "    id_parts = id.split(\".\")\n",
    "    \n",
    "    # Check if the last part of the ID ends with \"2\"\n",
    "    if id_parts[1] == '2':\n",
    "      \n",
    "        obs_data[0].append(id)\n",
    "    if id_parts[1] == '5':\n",
    "     \n",
    "        obs_data[1].append(id)\n",
    "    if id_parts[1] == '10':\n",
    "      \n",
    "        obs_data[2].append(id)\n",
    "    if id_parts[1] == '15':\n",
    "     \n",
    "        obs_data[3].append(id)\n",
    "# obs_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af10441-869d-483c-a88a-63d79a49d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counts\n",
    "\n",
    "TPR_recalls_google, FNRs_google, FPRs_google, F1s_google = [], [], [], []\n",
    "TPR_recalls_mapbox, FNRs_mapbox, FPRs_mapbox, F1s_mapbox = [], [], [], []\n",
    "TPR_recalls_ai, FNRs_ai, FPRs_ai, F1s_ai = [], [], [], []\n",
    "\n",
    "count = 0  # Reset count for each iteration\n",
    "\n",
    "\n",
    "for obs in range(len(obs_data)):\n",
    "\n",
    "    TP_ai, FN_ai, FP_ai, TN_ai = 0, 0, 0, 0\n",
    "    TP_mapbox, FN_mapbox, FP_mapbox, TN_mapbox = 0, 0, 0, 0\n",
    "    TP_google, FN_google, FP_google, TN_google = 0, 0, 0, 0\n",
    "    for id_str in obs_data[obs]:\n",
    "        for data_dict in data:\n",
    "            if data_dict['id'] == id_str:\n",
    "         \n",
    "                # Check if the highest scoring intent matches the intent_goal\n",
    "                if google_intention[id_str]:\n",
    "                    # print(data_dict['id'])\n",
    "                    TP_google += 1\n",
    "                    TN_google += len(intent_dic_google[id_str]) - 1\n",
    "                else:\n",
    "                    # print(data_dict['id'])\n",
    "                    FN_google += 1\n",
    "                    FP_google += len(intent_dic_google[id_str]) - 1\n",
    "                break \n",
    "\n",
    "        \n",
    "        for data_dict in data:\n",
    "            if data_dict['id'] == id_str:\n",
    "               \n",
    "                if mapbox_intention[id_str]:\n",
    "                    # print(data_dict['id'])\n",
    "                    TP_mapbox += 1\n",
    "                    TN_mapbox += len(intent_dic_mapbox[id_str]) - 1\n",
    "                else:\n",
    "                    # print(data_dict['id'])\n",
    "                    FN_mapbox += 1\n",
    "                    FP_mapbox += len(intent_dic_mapbox[id_str]) - 1\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        for data_dict in data:\n",
    "            if data_dict['id'] == id_str:\n",
    "                \n",
    "                if ai_intention[id_str]:\n",
    "                    # print(data_dict['id'])\n",
    "                    TP_ai += 1\n",
    "                    TN_ai += len(intent_dic_ai[id_str]) - 1\n",
    "                else:\n",
    "                    # print(data_dict['id'])\n",
    "                    FN_ai += 1\n",
    "                    FP_ai += len(intent_dic_ai[id_str]) - 1\n",
    "                break\n",
    "     \n",
    "\n",
    "        \n",
    "\n",
    " \n",
    "    # print(TP_ai)\n",
    "    # print(FN_ai)\n",
    "    TPR_recall_google = TP_google/(TP_google + FN_google)\n",
    "    Precision_google = TP_google/(TP_google + FP_google)\n",
    "    FNR_google = FN_google/(FN_google + TP_google)\n",
    "    FPR_google = FP_google/max(FP_google + TN_google, 1)\n",
    "    \n",
    "    F1s_google.append(round((2*Precision_google*TPR_recall_google)/(Precision_google + TPR_recall_google), 2))\n",
    "    TPR_recalls_google.append(TPR_recall_google)\n",
    "    FNRs_google.append(FNR_google)\n",
    "    FPRs_google.append(FPR_google)\n",
    "\n",
    "    # Calculate evaluation metrics, handle division by zero cases\n",
    "    TPR_recall_mapbox = TP_mapbox / (TP_mapbox + FN_mapbox) \n",
    "    Precision_mapbox = TP_mapbox / (TP_mapbox + FP_mapbox)  \n",
    "    FNR_mapbox = FN_mapbox / (FN_mapbox + TP_mapbox)  \n",
    "    FPR_mapbox = FP_mapbox/(FP_mapbox + TN_mapbox)\n",
    "    \n",
    "    # Calculate F1 score, handle division by zero cases\n",
    "    denominator = max(Precision_mapbox + TPR_recall_mapbox, 1)  # Ensuring the denominator is not zero\n",
    "\n",
    "    F1s_mapbox.append(round((2 * Precision_mapbox * TPR_recall_mapbox) / denominator, 2))\n",
    "    TPR_recalls_mapbox.append(TPR_recall_mapbox)\n",
    "    FNRs_mapbox.append(FNR_mapbox)\n",
    "    FPRs_mapbox.append(FPR_mapbox)\n",
    "\n",
    "\n",
    "    # Calculate evaluation metrics, handle division by zero cases\n",
    "    TPR_recall_ai = TP_ai / max(TP_ai + FN_ai, 1)  # Ensuring the denominator is not zero\n",
    "    Precision_ai = TP_ai / max(TP_ai + FP_ai, 1)  # Ensuring the denominator is not zero\n",
    "    FNR_ai = FN_ai / max(FN_ai + TP_ai, 1)  # Ensuring the denominator is not zero\n",
    "    \n",
    "    # Calculate FPR, handle division by zero cases\n",
    "    FPR_ai = FP_ai / max(FP_ai + TN_ai, 1)  # Ensuring the denominator is not zero\n",
    "    \n",
    "    # Calculate F1 score, handle division by zero cases\n",
    "    denominator = max(Precision_ai + TPR_recall_ai, 1)  # Ensuring the denominator is not zero\n",
    "\n",
    "    F1s_ai.append(round((2 * Precision_ai * TPR_recall_ai) / denominator, 2))\n",
    "    TPR_recalls_ai.append(TPR_recall_ai)\n",
    "    FNRs_ai.append(FNR_ai)\n",
    "    FPRs_ai.append(FPR_ai)\n",
    "\n",
    "# TPR_recalls_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc9547-02c8-49f1-88dc-3fc2f7fb741e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf47b7a-d4da-4120-98d2-d1554ca1950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR_recalls_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581316f8-b781-4f47-b85b-c0ca557be84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define observation labels\n",
    "observations = ['2', '5', '10', '15']\n",
    "observation_title = 'Goals'\n",
    "\n",
    "# Create DataFrames for each data source\n",
    "df_google = pd.DataFrame({\n",
    "    'TPR': TPR_recalls_google,\n",
    "    'FNR': FNRs_google,\n",
    "    'FPR': FPRs_google,\n",
    "    'F1': F1s_google\n",
    "}, index=observations)\n",
    "\n",
    "df_mapbox = pd.DataFrame({\n",
    "    'TPR': TPR_recalls_mapbox,\n",
    "    'FNR': FNRs_mapbox,\n",
    "    'FPR': FPRs_mapbox,\n",
    "    'F1': F1s_mapbox\n",
    "}, index=observations)\n",
    "\n",
    "df_ai = pd.DataFrame({\n",
    "    'TPR': TPR_recalls_ai,\n",
    "    'FNR': FNRs_ai,\n",
    "    'FPR': FPRs_ai,\n",
    "    'F1': F1s_ai\n",
    "}, index=observations)\n",
    "\n",
    "# Create an empty DataFrame as the separator row\n",
    "separator_row_1 = pd.DataFrame(' ', index=observations, columns=['Google'])\n",
    "separator_row_2 = pd.DataFrame(' ', index=observations, columns=['Mapbox'])\n",
    "separator_row_3 = pd.DataFrame(' ', index=observations, columns=['LLM'])\n",
    "\n",
    "\n",
    "# Concatenate the DataFrames with separator rows\n",
    "result_df = pd.concat([separator_row_1, df_google, separator_row_2, df_mapbox, separator_row_3, df_ai], axis=1)\n",
    "result_df = result_df.rename_axis(observation_title)\n",
    "# Display the result DataFrame\n",
    "# print(result_df)\n",
    "\n",
    "folder_path = 'data_output'\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Write DataFrame to a CSV file in the specified folder\n",
    "# result_df.to_csv(os.path.join(folder_path, 'result_dataframe_goal.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14451db1-632a-460f-948b-0f1b6aa86bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e0d55-7e25-464d-933c-f47a46dfe368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the x-axis values\n",
    "x = ['2', '5', '10', '15']\n",
    "\n",
    "# Plot TPR for Mapbox and AI\n",
    "plt.plot(x, TPR_recalls_google, label='Google TPR')\n",
    "plt.plot(x, TPR_recalls_mapbox, label='Mapbox TPR')\n",
    "plt.plot(x, TPR_recalls_ai, label='LLM TPR')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('No. Goals')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('True Positive Rate (TPR) - Mapbox vs LLM')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1adb0-69a0-49e8-b0ee-38bb42d1b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_data = [[] for _ in range(5)]\n",
    "\n",
    "for id in intent_dic_ai:\n",
    "    # Find the highest scoring intent\n",
    "\n",
    "    id_parts = id.split(\".\")\n",
    "    \n",
    "    # Check if the last part of the ID ends with \"2\"\n",
    "    if id_parts[-1] == '1':\n",
    "      \n",
    "        obs_data[0].append(id)\n",
    "    if id_parts[-1] == '3':\n",
    "     \n",
    "        obs_data[1].append(id)\n",
    "    if id_parts[-1] == '5':\n",
    "      \n",
    "        obs_data[2].append(id)\n",
    "    if id_parts[-1] == '10':\n",
    "     \n",
    "        obs_data[3].append(id)\n",
    "    if id_parts[-1] == '15':\n",
    "\n",
    "        obs_data[4].append(id)\n",
    "# obs_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45cff5d-0bc1-4a47-aa71-3c4b6298d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counts\n",
    "\n",
    "TPR_recalls_google, FNRs_google, FPRs_google, F1s_google = [], [], [], []\n",
    "TPR_recalls_mapbox, FNRs_mapbox, FPRs_mapbox, F1s_mapbox = [], [], [], []\n",
    "TPR_recalls_ai, FNRs_ai, FPRs_ai, F1s_ai = [], [], [], []\n",
    "\n",
    "count = 0  # Reset count for each iteration\n",
    "\n",
    "\n",
    "for obs in range(len(obs_data)):\n",
    "    TP_ai, FN_ai, FP_ai, TN_ai = 0, 0, 0, 0\n",
    "    TP_mapbox, FN_mapbox, FP_mapbox, TN_mapbox = 0, 0, 0, 0\n",
    "    TP_google, FN_google, FP_google, TN_google = 0, 0, 0, 0\n",
    "    for id_str in obs_data[obs]:\n",
    "        for data_dict in data:\n",
    "            if data_dict['id'] == id_str:\n",
    "         \n",
    "                # Check if the highest scoring intent matches the intent_goal\n",
    "                if google_intention[id_str]:\n",
    "                    # print(data_dict['id'])\n",
    "                    TP_google += 1\n",
    "                    TN_google += len(intent_dic_google[id_str]) - 1\n",
    "                else:\n",
    "                    # print(data_dict['id'])\n",
    "                    FN_google += 1\n",
    "                    FP_google += len(intent_dic_google[id_str]) - 1\n",
    "                break \n",
    "\n",
    "        \n",
    "        for data_dict in data:\n",
    "            if data_dict['id'] == id_str:\n",
    "               \n",
    "                if mapbox_intention[id_str]:\n",
    "                    # print(data_dict['id'])\n",
    "                    TP_mapbox += 1\n",
    "                    TN_mapbox += len(intent_dic_mapbox[id_str]) - 1\n",
    "                else:\n",
    "                    # print(data_dict['id'])\n",
    "                    FN_mapbox += 1\n",
    "                    FP_mapbox += len(intent_dic_mapbox[id_str]) - 1\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        for data_dict in data:\n",
    "            if data_dict['id'] == id_str:\n",
    "                \n",
    "                if ai_intention[id_str]:\n",
    "                    # print(data_dict['id'])\n",
    "                    TP_ai += 1\n",
    "                    TN_ai += len(intent_dic_ai[id_str]) - 1\n",
    "                else:\n",
    "                    # print(data_dict['id'])\n",
    "                    FN_ai += 1\n",
    "                    FP_ai += len(intent_dic_ai[id_str]) - 1\n",
    "                break\n",
    "\n",
    "        \n",
    "  \n",
    "    # print(TP_ai)\n",
    "    # print(FN_ai)\n",
    "    TPR_recall_google = TP_google/(TP_google + FN_google)\n",
    "    Precision_google = TP_google/(TP_google + FP_google)\n",
    "    FNR_google = FN_google/(FN_google + TP_google)\n",
    "    FPR_google = FP_google/max(FP_google + TN_google, 1)\n",
    "    \n",
    "    F1s_google.append(round((2*Precision_google*TPR_recall_google)/(Precision_google + TPR_recall_google), 2))\n",
    "    TPR_recalls_google.append(TPR_recall_google)\n",
    "    FNRs_google.append(FNR_google)\n",
    "    FPRs_google.append(FPR_google)\n",
    "\n",
    "    # Calculate evaluation metrics, handle division by zero cases\n",
    "    TPR_recall_mapbox = TP_mapbox / (TP_mapbox + FN_mapbox) \n",
    "    Precision_mapbox = TP_mapbox / (TP_mapbox + FP_mapbox)  \n",
    "    FNR_mapbox = FN_mapbox / (FN_mapbox + TP_mapbox)  \n",
    "    FPR_mapbox = FP_mapbox/(FP_mapbox + TN_mapbox)\n",
    "    \n",
    "    # Calculate F1 score, handle division by zero cases\n",
    "    denominator = max(Precision_mapbox + TPR_recall_mapbox, 1)  # Ensuring the denominator is not zero\n",
    "\n",
    "    F1s_mapbox.append(round((2 * Precision_mapbox * TPR_recall_mapbox) / denominator, 2))\n",
    "    TPR_recalls_mapbox.append(round(TPR_recall_mapbox, 2))\n",
    "    FNRs_mapbox.append(round(FNR_mapbox,2))\n",
    "    FPRs_mapbox.append(round(FPR_mapbox,2))\n",
    "\n",
    "\n",
    "    # Calculate evaluation metrics, handle division by zero cases\n",
    "    TPR_recall_ai = TP_ai / max(TP_ai + FN_ai, 1)  # Ensuring the denominator is not zero\n",
    "    Precision_ai = TP_ai / max(TP_ai + FP_ai, 1)  # Ensuring the denominator is not zero\n",
    "    FNR_ai = FN_ai / max(FN_ai + TP_ai, 1)  # Ensuring the denominator is not zero\n",
    "    \n",
    "    # Calculate FPR, handle division by zero cases\n",
    "    FPR_ai = FP_ai / max(FP_ai + TN_ai, 1)  # Ensuring the denominator is not zero\n",
    "    \n",
    "    # Calculate F1 score, handle division by zero cases\n",
    "    denominator = max(Precision_ai + TPR_recall_ai, 1)  # Ensuring the denominator is not zero\n",
    "\n",
    "    F1s_ai.append(round((2 * Precision_ai * TPR_recall_ai) / denominator, 2))\n",
    "    TPR_recalls_ai.append(round(TPR_recall_ai,2))\n",
    "    FNRs_ai.append(round(FNR_ai,2))\n",
    "    FPRs_ai.append(round(FPR_ai,2))\n",
    "\n",
    "# TPR_recalls_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f5274-429f-48d3-a961-a37cdcadcb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define observation labels\n",
    "observations = ['1', '3', '5', '10', '15']\n",
    "observation_title = 'No. obs'\n",
    "\n",
    "# Create DataFrames for each data source\n",
    "df_google = pd.DataFrame({\n",
    "    'TPR': TPR_recalls_google,\n",
    "    'FNR': FNRs_google,\n",
    "    'FPR': FPRs_google,\n",
    "    'F1': F1s_google\n",
    "}, index=observations)\n",
    "\n",
    "df_mapbox = pd.DataFrame({\n",
    "    'TPR': TPR_recalls_mapbox,\n",
    "    'FNR': FNRs_mapbox,\n",
    "    'FPR': FPRs_mapbox,\n",
    "    'F1': F1s_mapbox\n",
    "}, index=observations)\n",
    "\n",
    "df_ai = pd.DataFrame({\n",
    "    'TPR': TPR_recalls_ai,\n",
    "    'FNR': FNRs_ai,\n",
    "    'FPR': FPRs_ai,\n",
    "    'F1': F1s_ai\n",
    "}, index=observations)\n",
    "\n",
    "# Create an empty DataFrame as the separator row\n",
    "separator_row_1 = pd.DataFrame(' ', index=observations, columns=['Google'])\n",
    "separator_row_2 = pd.DataFrame(' ', index=observations, columns=['Mapbox'])\n",
    "separator_row_3 = pd.DataFrame(' ', index=observations, columns=['LLM'])\n",
    "\n",
    "\n",
    "# Concatenate the DataFrames with separator rows\n",
    "result_df = pd.concat([separator_row_1, df_google, separator_row_2, df_mapbox, separator_row_3, df_ai], axis=1)\n",
    "result_df = result_df.rename_axis(observation_title)\n",
    "# Display the result DataFrame\n",
    "# print(result_df)\n",
    "# Specify the folder path\n",
    "folder_path = 'data_output'\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Write DataFrame to a CSV file in the specified folder\n",
    "# result_df.to_csv(os.path.join(folder_path, 'result_dataframe_obs.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d450727-3b1c-41bd-8154-a038ea11d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae5be1-820a-4ccb-89cc-e8fcfad2c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the x-axis values\n",
    "x = ['1', '3', '5', '10', '15']\n",
    "\n",
    "# Plot TPR for Mapbox and LLM\n",
    "plt.plot(x, TPR_recalls_google, label='Google TPR')\n",
    "plt.plot(x, TPR_recalls_mapbox, label='Mapbox TPR')\n",
    "plt.plot(x, TPR_recalls_ai, label='LLM TPR')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('No. Obs')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('True Positive Rate (TPR) - Mapbox vs LLM')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce01bd7-f312-43c1-a80d-bbb4e3490b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee99e5cf-57c5-47c4-b811-4f8dd8da946f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
